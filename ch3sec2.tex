\documentclass{article}
\usepackage{amsmath, amssymb, amsthm, graphicx}
\usepackage[export]{adjustbox}

\title{Chapter 3 Section 2}
\author{Andrew Taylor}
\date{April 30 2022}
\newtheorem{theorem}{Theorem}
\newtheorem{problem}{Problem}
\newtheorem*{solution}{Solution}
\newcommand{\rref}[1]{\mathrm{rref \, #1}}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Rank}{rank}
\DeclareMathOperator{\im}{im}

\begin{document}
\maketitle

\begin{problem}
Is $W = \Bigg \{ \begin{bmatrix} x \\ y \end{bmatrix}$ in $\mathbb{R}^2$: $x \geq 0$ and $y \geq 0 \Bigg \}$ a subspace of $\mathbb{R}^2$?
\end{problem}

\begin{solution} 
W contains the zero vector and is closed under addition. But W is not closed under scalar multiplication. Therefore W is not a subspace of $\mathbb{R}^2$.
\end{solution}

\begin{problem}
Show that the only subspaces of $\mathbb{R}^2$ are $\mathbb{R}^2$ itself, the set $\{ \vec{0} \}$, and any of the lines through the origin.
\end{problem}

\begin{solution}
Let W be a subspace of $\mathbb{R}^2$ that is neither a line through the origin nor the set $\{ \vec{0} \}$. Then we can choose two nonzero nonparallel vectors $\vec{v} = \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix}$ and $\vec{w} = \begin{bmatrix} w_{1} \\ w_{2} \end{bmatrix}$ from our subspace W.  Let $\vec{u} = \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix}$ be a vector in $\mathbb{R}^2$. We will show that we can write $\vec{u}$ as a linear combination of $\vec{v}$ and $\vec{w}$. \\

If $\vec{u}$ can be written as a linear combination of $\vec{v}$ and $\vec{w}$, then there are solutions to the equation

\begin{align*}
x_{1} \vec{v} + x_{2} \vec{w} = \vec{u}
\end{align*}

where $x_{1}$ and $x_{2}$ are real numbers. We can write this equation in matrix form

\begin{align*}
\begin{bmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} = \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix} 
\end{align*}

This equation has solutions when $A = \begin{bmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{bmatrix}$ is invertible. We know that A is invertible when $\det A$ is nonzero. \\

The components $v_{1}, v_{2}, w_{1}, w_{2}$ can either be zero or nonzero. There is a small number of possible cases, since both vectors are not the zero vector, and since the two 
vectors are not parallel. 
\\ \\
\emph{Case 1: } $v_{1} = 0, v_{2} \neq 0, w_{1} \neq 0, w_{2} = 0$ \\
\emph{Case 2: } $v_{1} \neq 0, v_{2} = 0, w_{1} = 0, w_{2} \neq 0$ \\

In both of these cases, the determinant of A is nonzero, and the matrix A is invertible.
\\ \\
\emph{Case 3: } At least one of the vectors ($\vec{v}$ and $\vec{w}$) has two nonzero components. \\

Let $\vec{v}$ be the vector with two nonzero components. \\

There exist real numbers $c_{1}$ and $c_{2}$ such that $c_{1} v_{1} = w_{1}$ and $c_{2} v_{2} = w_{2}$. We know that $c_{1} \neq c_{2}$ since the two vectors are not scalar multiples of each other. We can substitute these expressions when we calculate the determinant of A.

\begin{align*}
\det A &= v_{1} w_{2} - v_{2} w_{1} \\
&= v_{1} (c_{2} v_{2}) - v_{2} (c_{1} v_{1}) \\
&= c_{2} v_{1} v_{2} - c_{1} v_{1} v_{2} \\
&= v_{1} v_{2}  (c_{2} - c_{1}) 
\end{align*} 

Since $v_{1} \neq 0$, $v_{2} \neq 0$ and $c_{2} \neq c_{1}$, the determinant of A is nonzero. Thus the matrix A is invertible, and the equation 

\begin{align*}
x_{1} \vec{v} + x_{2} \vec{w} = \vec{u}
\end{align*}

has solutions for $x_{1}$ and $x_{2}$. \\

Since W is closed under linear combinations, the vector $\vec{u}$ is in the subspace W. This means that W contains every real number, so $W = \mathbb{R}^2$. \\

We can also express this using a linear transformation. Let $T : \mathbb{R}^2 \to \mathbb{R}^2$ be the linear transformation

\begin{align*}
T(\vec{x}) &= \begin{bmatrix} \vec{v} & \vec{w} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix} \\
&= \begin{bmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \end{bmatrix}
\end{align*}

We have shown that the matrix $\begin{bmatrix} v_{1} & w_{1} \\ v_{2} & w_{2} \end{bmatrix}$ is invertible. \\

This means that there is a unique solution $\vec{x}$ for every vector $\vec{u}$ in $\mathbb{R}^2$. \\

This is equivalent to saying any vector $\vec{u}$ in $\mathbb{R}^2$ can be written as a linear combination of $\vec{v}$ and $\vec{w}$.

\end{solution}

\begin{problem}
Consider the plane V in $\mathbb{R}^3$ given by the equation

\begin{align*} x_{1} + 2x_{2} + 3x_{3} = 0 \end{align*}

\begin{itemize}
\item Find a matrix A such that $V = \ker A$
\item Find a matrix B such that $V = \im B$
\end{itemize}
\end{problem}

\begin{solution}
Let $A = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}$. We can write 

\begin{align*}
V &= \ker A \\ &= \ker \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}
\end{align*}

We can find two nonparallel vectors $\vec{v}$ and $\vec{w}$ in V. \\

Let $\vec{v} = \begin{bmatrix} 1 \\ 1 \\ -1 \end{bmatrix}$ and $\vec{w} = \begin{bmatrix} 1 \\ -2 \\ 1 \end{bmatrix}$. \\

We can construct our matrix B from the column vectors $\vec{v}$ and $\vec{w}$.

\begin{align*}
B = \begin{bmatrix} \vec{v} & \vec{w} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & -2 \\ -1 & 1 \end{bmatrix} 
\end{align*}

Now we can say 

\begin{align*}
V &= \im B \\ &= \im \begin{bmatrix} 1 & 1 \\ 1 & -2 \\ -1 & 1 \end{bmatrix} 
\end{align*}

\end{solution}

\begin{problem}
Consider the matrix

\begin{align*}
A = \begin{bmatrix} 1 & 2 & 1 & 2 \\ 1 & 2 & 2 & 3 \\ 1 & 2 & 3 & 4 \end{bmatrix}
\end{align*}

Find vectors in $\mathbb{R}^3$ that span the image of A. What is the smallest number of vectors needed to span the image of A?
\end{problem}

\begin{solution}
The vectors 

\begin{align*}
\vec{v_{1}} = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} 
\vec{v_{2}} = \begin{bmatrix} 2 \\ 2 \\ 2 \end{bmatrix}
\vec{v_{3}} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\vec{v_{4}} = \begin{bmatrix} 2 \\ 3 \\ 4 \end{bmatrix}
\end{align*}

span the image of A. \\

The vector $v_{2}$ is redundant because $v_{2} = 2v_{1}$. The vector $v_{4}$ is redundant because $v_{1} + v_{3} = v_{4}$. Thus only two vectors are needed to span the image of A, the vectors $v_{1}$ and $v_{3}$. 

\begin{align*}
\im A = \Span \left( v_{1}, v_{3} \right)
\end{align*}

We can verify this algebraically. We know that the image of A is spanned by the vectors $\vec{v_{1}}$, $\vec{v_{2}}$, $\vec{v_{3}}$, $\vec{v_{4}}$. \\

Let $\vec{u}$ be a vector in $\mathbb{R}^3$. Then

\begin{align*}
\vec{u} = c_{1} \vec{v_{1}} + c_{2} \vec{v_{2}} + c_{3} \vec{v_{3}} + c_{4} \vec{v_{4}}
\end{align*}

for some real coefficients $c_{1}, c_{2}, c_{3}, c_{4}$. Substituting we get 

\begin{align*}
\vec{u} &= c_{1} \vec{v_{1}} + c_{2} (2 \vec{v_{1}}) + c_{3} \vec{v_{3}} + c_{4} (v_{1} + v_{3}) \\
&= (c_{1} + c_{4} + 2c_{1}) \vec{v_{1}} + (c_{3} + c_{4}) \vec{v_{3}}
\end{align*}

Thus $\vec{u}$ is a linear combination of vectors $\vec{v_{1}}$ and $\vec{v_{3}}$. The vectors $\vec{v_{1}}$ and $\vec{v_{3}}$ form a basis for the image of A.

\end{solution}

\begin{problem}
Are the following vectors in $\mathbb{R}^7$ linearly independent?

\begin{align*}
\vec{v_{1}} = \begin{bmatrix} 7 \\ 0 \\ 4 \\ 0 \\ 1 \\ 9 \\ 0 \end{bmatrix}
\vec{v_{2}} = \begin{bmatrix} 6 \\ 0 \\ 7 \\ 1 \\ 4 \\ 8 \\ 0 \end{bmatrix}
\vec{v_{3}} = \begin{bmatrix} 5 \\ 0 \\ 6 \\ 2 \\ 3 \\ 1 \\ 7 \end{bmatrix}
\vec{v_{4}} = \begin{bmatrix} 4 \\ 5 \\ 3 \\ 3 \\ 2 \\ 2 \\ 4 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
$\vec{v_{2}}$ cannot be a linear combination of $\vec{v_{1}}$, since $\vec{v_{1}}$ has a 0 in the fourth component and $\vec{v_{2}}$ has a 1 in the fourth component. $\vec{v_{3}}$ cannot be a linear combination of $\vec{v_{1}}$ and $\vec{v_{2}}$, since $\vec{v_{1}}$ and $\vec{v_{2}}$ have zeros in the last component, and $\vec{v_{3}}$ has a seven in the last component. $\vec{v_{4}}$ cannot be a linear combination of $\vec{v_{1}}$, $\vec{v_{2}}$, and $\vec{v_{3}}$, since $\vec{v_{4}}$ has a 5 in the second component and the other three vectors all have zero in the second component. \\
\\
Thus the four vectors are linearly independent.
\end{solution}

\begin{problem}
Are the following vectors in $\mathbb{R}^7$ linearly independent?

\begin{align*}
\vec{v_{1}} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\vec{v_{2}} = \begin{bmatrix} 4 \\ 5 \\ 6 \end{bmatrix}
\vec{v_{3}} = \begin{bmatrix} 7 \\ 8 \\ 9 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The vectors $\vec{v_{1}}, \vec{v_{2}}, \vec{v_{3}}$ are not linearly independent since $2\vec{v_{2}} - \vec{v_{1}} = \vec{v_{3}}$.
\end{solution}

\begin{problem}
Suppose the column vectors of an $n \times m$ matrix A are linearly independent. Find the kernel of matrix A.
\end{problem}

\begin{solution}
Let 

\begin{align*} 
A = \begin{bmatrix} \vec{v_{1}} & \vec{v_{2}} & \cdots & \vec{v_{m}} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{m} \end{bmatrix}
\end{align*}

We can find the kernel of A by solving the equation 

\begin{align*} 
x_{1} \vec{v_{1}} + x_{2} \vec{v_{2}} + \cdots + x_{m} \vec{v_{m}} = 0
\end{align*}

Since the vectors $\vec{v_{1}}, \vec{v_{2}}, \ldots, \vec{v_{m}}$ are linearly independent, there is only the trivial relation, with $x_{1} = x_{2} = \cdots = x_{m} = 0$, thus the kernel of A is $\{\vec{0}\}$.

\end{solution}

\begin{problem}
If $v_{1}, \ldots, v_{m}$ is a basis of a subspace V of $\mathbb{R}^n$, and if $\vec{v}$ is a vector in V, how many solutions $c_{1},\ldots,c_{m}$ does the equation 

\begin{align*}
\vec{v} = c_{1} \vec{v_{1}} + \cdots + c_{m} \vec{v_{m}}
\end{align*}

have?
\end{problem}

\begin{solution}
Let $c_{1},\ldots,c_{m}$ and $d_{1},\ldots,d_{m}$ be solutions to the equation. Then

\begin{align*}
\vec{v} = c_{1} \vec{v_{1}} + \cdots + c_{m} \vec{v_{m}} \\
\vec{v} = d_{1} \vec{v_{1}} + \cdots + d_{m} \vec{v_{m}} 
\end{align*}

Subtracting we get

\begin{align*}
0 = (c_{1} - d_{1}) \vec{v_{1}} + \cdots + (c_{m} - d_{m}) \vec{v_{m}}
\end{align*}

Since the vectors $v_{1}, \ldots, v_{m}$ are linearly independent, we can conclude that $c_{1} = d_{1}$. Thus there is only one solution to the equation. 

\end{solution}

In the following three problems, which of the sets W are subspaces of $\mathbb{R}^3$?

\begin{problem}
\begin{align*}
W = \Bigg \{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} : x + y + z = 1 \Bigg \}
\end{align*}
\end{problem}

\begin{solution}
\begin{align*}
W = \Bigg \{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} : x + y + z = 1 \Bigg \}
\end{align*}
is not a subspace because it does not contain the zero vector.
\end{solution}

\begin{problem}
\begin{align*}
W = \Bigg \{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} : x \leq y \leq z \Bigg \}
\end{align*}
\end{problem}

\begin{solution}
\begin{align*}
W = \Bigg \{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} : x \leq y \leq z \Bigg \}
\end{align*}

is not a subspace since it is not closed under scalar multiplication.
\end{solution}

\begin{problem}
\begin{align*}
W = \Bigg \{ \begin{bmatrix} x + 2y + 3z \\ 4x + 5y + 6z \\ 7x + 8y + 9z \end{bmatrix} : \textrm{x, y, z are arbitrary constants} \Bigg \}
\end{align*}
\end{problem}

\begin{solution}
The first two column vectors are linearly independent, and the third is redundant. Thus W is the span of the first two column vectors, a plane passing through the origin. A plane that passes through the origin contains the zero vector, is closed under addition, and is closed under scalar multiplication. Thus W is a subspace of $\mathbb{R}^3$.
\end{solution}

\begin{problem}
Consider the vectors $\vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{m}}$ in $\mathbb{R}^n$. Is $\Span \left( \vec{v_{1}}, \dots, \vec{v_{m}} \right)$ necessarily a subspace of $\mathbb{R}^n$? Justify your answer.
\end{problem}

\begin{solution}
The span of $\vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{m}}$ is necessarily a subspace of $\mathbb{R}^n$. The span of $\vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{m}}$ is the set of all linear combinations of these vectors, which means it has to be closed under addition and multiplication. Furthermore, the zero vector is included in the span of these vectors, since we can multiply any vector by the scalar zero to get the zero vector. Thus the span of $\vec{v_{1}}, \vec{v_{2}}, \dots, \vec{v_{m}}$ is a subspace of $\mathbb{R}^n$, as it meets every requirement of a subspace.
\end{solution}

\begin{problem}
Give a geometrical description of all subspaces of $\mathbb{R}^3$. Justify your answer.
\end{problem}

\begin{solution}
The subspaces of $\mathbb{R}^3$ can be $\mathbb{R}^3$ itself, a plane passing through the origin, a line passing through the origin, or the set $\{ \vec{0} \}$.
\end{solution}

\begin{problem}
Consider two subspaces V and W of $\mathbb{R}^n$. Is the intersection $V \cap W$ necessarily a subspace of $\mathbb{R}^n$?
\end{problem}

\begin{solution}
Let $\vec{v}, \vec{w}$ be vectors in $V \cap W$. By the definition of intersection, the vectors $\vec{v}, \vec{w}$ are in V and W. Since V and W are closed under addition, the vector $\vec{v} + \vec{w}$ is in both V and W. Thus $\vec{v} + \vec{w}$ is in $V \cap W$. This means that $V \cap W$ is closed under addition. \\

Let $k \in \mathbb{R}$ and let $\vec{v} \in V \cap W$. We know from the definition of intersection that $\vec{v} \in V$ and $\vec{v} \in W$. Since V and W are closed under scalar multiplication, the vector $k \vec{v}$ is in both V and W. Thus the vector $k \vec{v}$ is in $V \cap W$. This means that $V \cap W$ is closed under scalar multiplication. \\

Since both V and W are subspaces, both V and W must contain the zero vector. Thus the zero vector is also in the intersection $V \cap W$. \\ 

The intersection $V \cap W$ is closed under addition, is closed under scalar multiplication, and contains the zero vector. Thus $V \cap W$ is a subspace of $\mathbb{R}^n$. 
\end{solution}

\begin{problem}
Consider two subspaces V and W of $\mathbb{R}^n$. Is the union $V \cup W$ necessarily a subspace of $\mathbb{R}^n$?
\end{problem}

\begin{solution}
The union of two subspaces is not necessarily a subspace. \\
\\
Let $V = \Span \left[1, 0 \right]$ and $W = \Span \left[0, 1 \right]$. The vectors $\left[1, 0 \right]$ and $\left[0, 1 \right]$ are in the union of V and W, but their sum $\left[1, 1 \right]$ is not in the union of V and W. The set $V \cup W$ is not closed under addition, so $V \cup W$ is not a subspace.
\end{solution}

\begin{problem}
Consider a nonempty subset W of $\mathbb{R}^n$ that is closed under addition and under scalar multiplication. Is W necessarily a subspace of $\mathbb{R}^n$? Explain
\end{problem}

\begin{solution}
W is necessarily a subspace of $\mathbb{R}^n$. Let $\vec{v}$ be a vector in W. Since W is closed under scalar multiplication, the additive inverse $-\vec{v}$ is in W. Since W is closed under addition, $\vec{v} + -\vec{v} = \vec{0}$ is in W. Since W contains the zero vector and is closed under addition and scalar multiplication, W is a subspace of $\mathbb{R}^n$.
\end{solution}

\begin{problem}
Find a nontrivial relation among the following vectors:

\begin{align*}
\begin{bmatrix}1 \\ 2 \end{bmatrix}, \begin{bmatrix} 2 \\ 3 \end{bmatrix}, \begin{bmatrix}3 \\ 4 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
\begin{align*}
2* \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix}1 \\ 2 \end{bmatrix} - \begin{bmatrix}3 \\ 4 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
\end{align*}
\end{solution}

\begin{problem}
Consider the vectors $\vec{v_{1}}, \vec{v_{2}}, \ldots, \vec{v_{m}}$ in $\mathbb{R}^n$, with $\vec{v_{m}} = \vec{0}$. Are these vectors linearly independent?
\end{problem}

\begin{solution}
The vectors are not linearly independent, since we can form a nontrivial relation by setting $c_{1} = c_{2} = \cdots = c_{m-1} = 0$ and $c_{m} = 1$.

\begin{align*}
c_{1} \vec{v_{1}} + c_{2} \vec{v_{2}} + \cdots + c_{m} \vec{v_{m}} = \vec{0}
\end{align*}

is a nontrivial relation, where $c_{1} = c_{2} = \cdots = c_{m-1} = 0$ and $c_{m} = 1$
\end{solution}

In the following exercises, identify the redundant vectors and determine whether the given vectors are linearly independent.

\begin{problem}
\begin{align*}
\begin{bmatrix} 7 \\ 11 \end{bmatrix}, 
\begin{bmatrix} 0 \\ 0 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The two vectors are not linearly independent, as the zero vector is redundant. We can write the zero vector as a linear combination of $\begin{bmatrix} 7 \\ 11 \end{bmatrix}$, thus the zero vector is redundant.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 7 \\ 11 \end{bmatrix}, 
\begin{bmatrix} 11 \\ 7 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The two vectors are linearly independent, as we cannot write the second vector as a linear combination of the first.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 2 \\ 1 \end{bmatrix}, 
\begin{bmatrix} 6 \\ 3 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The two vectors are not linearly independent. The second vector is redundant: the second vector is a scalar multiple of the first.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 2 \\ 1 \end{bmatrix}, 
\begin{bmatrix} 6 \\ 3 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The two vectors are not linearly independent. The second vector is redundant.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, 
\begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix},
\begin{bmatrix} 6 \\ 5 \\ 4 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The three vectors are not linearly independent, as a nontrivial relation exists among them.

\begin{align*} 
\begin{bmatrix} 6 \\ 5 \\ 4 \end{bmatrix} - \begin{bmatrix} 3 \\ 2 \\ 1 \end{bmatrix} - 3 \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} = \vec{0}
\end{align*}

The third vector is redundant.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 2 \end{bmatrix}, 
\begin{bmatrix} 2 \\ 3 \end{bmatrix},
\begin{bmatrix} 3 \\ 4 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The three vectors are linearly dependent. The third vector is redundant, as shown.

\begin{align*} 
2 * \begin{bmatrix} 2 \\ 3 \end{bmatrix} - \begin{bmatrix} 3 \\ 4 \end{bmatrix} - \begin{bmatrix} 1 \\ 2 \end{bmatrix} = \vec{0}
\end{align*}
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix},
\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The three vectors are linearly independent. The zero components of each vector show that no vector is redundant.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix},
\begin{bmatrix} 1 \\ 3 \\ 6 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
Let $A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 3 \\ 1 & 3 & 6 \end{bmatrix}$. Let's find the kernel of A.

\begin{align*}
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 1 & 2 & 3 & 0 \\ 1 & 3 & 6 & 0\end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 0 & 1 & 2 & 0 \\ 0 & 2 & 5 & 0\end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 0 & 1 & 2 & 0 \\ 0 & 0 & 1 & 0\end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0\end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0\end{array} \right) 
\end{align*}

The kernel of A is $\{ \vec{0} \}$. Thus the three vectors are linearly independent, and none of the vectors are redundant.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix} 3 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 4 \\ 5 \\ 0 \end{bmatrix},
\begin{bmatrix} 6 \\ 7 \\ 0 \end{bmatrix},
\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The first vector (the zero vector) is redundant. The third vector (3, 0, 0) is redundant. The fifth and sixth vectors (4, 5, 0) and (6, 7, 0) are redundant. The seven vectors are linearly dependent.\\
\\
The vectors (1, 0, 0) (0, 1, 0) and (0, 0, 1) are linearly independent.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 2 \\ 0 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \end{bmatrix},
\begin{bmatrix} 0 \\ 0 \\ 1 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 3 \\ 4 \\ 5 \\ 0 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The second and fifth vectors are redundant. The five vectors are linearly dependent.
\end{solution}

\begin{problem}
\begin{align*}
\begin{bmatrix} 1 \\ 1 \\ 1 \\ 1 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix}, 
\begin{bmatrix} 1 \\ 4 \\ 7 \\ 10 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
Let $A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 7 \\ 1 & 4 & 10 \end{bmatrix}$ \\
\\ \\
Let's find the kernel of A.

\begin{align*}
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 1 & 2 & 4 & 0 \\ 1 & 3 & 7 & 0 \\ 1 & 4 & 10 & 0 \end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 2 & 6 & 0 \\ 0 & 3 & 9 & 0 \end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 1 & 1 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array} \right) \\
\left( \begin{array}{ccc|c} 1 & 0 & -2 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \end{array} \right) \\
\end{align*}

This gives us the equations $x_{1} = 2x_{3}$ and $x_{2} = -3x_{3}$. 

\begin{align*}
\ker A &= \begin{bmatrix} 2t \\ -3t \\ t \end{bmatrix} \\
&= t \begin{bmatrix} 2 \\ -3 \\ 1 \end{bmatrix} \\
&= \Span \begin{bmatrix} 2 \\ -3 \\ 1 \end{bmatrix}
\end{align*}

The kernel of A is the line spanned by the vector $\begin{bmatrix} 2 \\ -3 \\ 1 \end{bmatrix}$. \\

This means that the three column vectors are linearly dependent, since there are many nontrivial relations among the three vectors. The third column vector is redundant.

\end{solution}

In the following problems find a redundant column vector of the given matrix A, and write it as a linear combination of the preceding columns. Use this to find a nonzero vector in the kernel of A.

\begin{problem}
\begin{align*}
A = \begin{bmatrix}1 & 1 \\ 1 & 1 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
Let

\begin{align*}
\begin{bmatrix}\vec{v} & \vec{w} \end{bmatrix} = \begin{bmatrix}1 & 1 \\ 1 & 1 \end{bmatrix}
\end{align*}

We're looking for a linear combination of $\vec{v}$ and $\vec{w}$ that is nontrivial. In other words, we want a solution to the equation

\begin{align*}
x_{1} \vec{v} + x_{2} \vec{w} = 0
\end{align*}

that is not $(0, 0)$. \\

First we can identify that the second column vector is redundant. This means there is a nontrivial relation, because the vectors are linearly dependent. \\

The vector (1, -1) is a nonzero vector in the kernel of A because 

\begin{align*}
1\begin{bmatrix} 1 \\ 1 \end{bmatrix} - 1\begin{bmatrix} 1 \\ 1\end{bmatrix} = 0
\end{align*}

\end{solution}

\begin{problem}
\begin{align*}
A = \begin{bmatrix}1 & 3 \\ 2 & 6 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
Let

\begin{align*}
\begin{bmatrix}\vec{v} & \vec{w} \end{bmatrix} = \begin{bmatrix}1 & 3 \\ 2 & 6 \end{bmatrix}
\end{align*}

We're looking for a linear combination of $\vec{v}$ and $\vec{w}$ that is nontrivial. In other words, we want a solution to the equation

\begin{align*}
x_{1} \vec{v} + x_{2} \vec{w} = 0
\end{align*}

that is not $(0, 0)$. \\

First we can identify that the second column vector is redundant. This means there is a nontrivial relation, because the vectors are linearly dependent. \\

The vector (3, -1) is a nonzero vector in the kernel of A because 

\begin{align*}
3 \begin{bmatrix} 1 \\ 1 \end{bmatrix} - 1\begin{bmatrix} 1 \\ 1\end{bmatrix} = 0
\end{align*}

\end{solution}

\begin{problem}
\begin{align*}
A = \begin{bmatrix}0 & 1 \\ 0 & 2 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The first vector is redundant, because it's the zero vector. Let $t \in \mathbb{R}$ such that $t \neq 0$. Every vector of the form $(t, 0)$ is a nontrivial vector in the kernel of A. Thus the kernel of A is a line. 
\end{solution}

\begin{problem}
\begin{align*}
A = \begin{bmatrix}1 & 0 & 2 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The third column vector is redundant, because it can be written as a linear combination of the first two vectors. We can write the nontrivial relation

\begin{align*}
2 \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} - 3 \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} - 1 \begin{bmatrix} 2 \\ 3 \\ 0 \end{bmatrix} + 0 \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}
\end{align*}

which gives us a nonzero vector in the kernel of A 

\begin{align*}
\vec{v} = \begin{bmatrix} 2 \\ -3 \\ -1 \\ 0 \end{bmatrix}
\end{align*}
\end{solution}

\begin{problem}
\begin{align*}
A = \begin{bmatrix}1 & 0 & 1 \\ 1 & 1 & 1 \\ 1 & 0 & 1 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
The third column vector is redundant. The vector $\vec{v} = \begin{bmatrix} 1 \\ 0 \\ -1 \end{bmatrix}$ is a nontrivial vector in the kernel of A.
\end{solution}

\begin{problem}
\begin{align*}
A = \begin{bmatrix}1 & 3 & 6 \\ 1 & 2 & 5 \\ 1 & 1 & 4 \end{bmatrix}
\end{align*}
\end{problem}

\begin{solution}
We can find out if there are any redundant vectors by computing the kernel of the matrix. We start by calculating $\rref{A}$.

\begin{align*}
& \left( \begin{array}{ccc|c}1 & 3 & 6 & 0 \\ 1 & 2 & 5 & 0 \\ 1 & 1 & 4 & 0\end{array} \right) \\
& \left( \begin{array}{ccc|c}0 & 2 & 2 & 0 \\ 0 & 1 & 1 & 0 \\ 1 & 1 & 4 & 0\end{array} \right) \\
& \left( \begin{array}{ccc|c}0 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 1 & 1 & 4 & 0\end{array} \right) \\
& \left( \begin{array}{ccc|c}0 & 0 & 0 & 0 \\ 0 & 1 & 1 & 0 \\ 1 & 0 & 3 & 0\end{array} \right)
\end{align*}

This gives us the equations

\begin{align*}
x_{1} = -3x_{3} \\
x_{2} = -2x_{3}
\end{align*}

Thus the kernel of A is a line given by

\begin{align*}
\ker A &= \begin{bmatrix} -3t \\ -2t \\ t \end{bmatrix} \\
&= t \begin{bmatrix} -3 \\ -2 \\ 1 \end{bmatrix} \\
&= \Span \begin{bmatrix} -3 \\ -2 \\ 1 \end{bmatrix} \\
\end{align*}

A nonzero kernel means the three vectors are linearly dependent. In other words, one of the vectors has to be redundant. Since the second vector is not a scalar multiple of the first, the first two vectors are linearly independent. That means the third vector has to be redundant.

\end{solution}

\end{document}